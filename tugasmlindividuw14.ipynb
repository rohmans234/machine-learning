{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =============================================================================\n# STEP 1: SETUP AND IMPORTS\n# =============================================================================\n# This cell installs necessary libraries, downloads the dataset, and imports modules.\n\n!pip install torch torchtext==0.17.0 tqdm sacrebleu -q\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport unicodedata\nimport re\nimport random\nfrom collections import Counter\nfrom tqdm import tqdm\nimport math\nimport time\nimport sacrebleu\n\n# Download and extract the English-Indonesian dataset\n!wget -q http://www.manythings.org/anki/ind-eng.zip\n!unzip -q ind-eng.zip\n\nprint(\"✅ Setup Complete. Dataset is ready.\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:27:15.495202Z","iopub.execute_input":"2025-08-21T16:27:15.495785Z","iopub.status.idle":"2025-08-21T16:30:10.245560Z","shell.execute_reply.started":"2025-08-21T16:27:15.495757Z","shell.execute_reply":"2025-08-21T16:30:10.244593Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m892.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchtune 0.6.1 requires torchdata==0.11.0, but you have torchdata 0.7.1 which is incompatible.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.2.0 which is incompatible.\ntorchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m✅ Setup Complete. Dataset is ready.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# =============================================================================\n# STEP 2: DATA PREPARATION\n# =============================================================================\n# This section contains all functions for loading, cleaning, and preparing the data.\n\n# --- Define special tokens and their indices ---\nSPECIALS = [\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"]\nPAD_IDX, BOS_IDX, EOS_IDX, UNK_IDX = 0, 1, 2, 3\n\ndef normalize_and_tokenize(s: str):\n    \"\"\"Cleans and tokenizes a string.\"\"\"\n    s = s.lower().strip()\n    # Add space before punctuation\n    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n    # Replace non-alphanumeric characters with spaces\n    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n    return s.split()\n\ndef load_pairs(path, max_pairs=10000):\n    \"\"\"Loads and tokenizes sentence pairs from a file.\"\"\"\n    pairs = []\n    with open(path, encoding=\"utf-8\") as f:\n        for line in f:\n            cols = line.rstrip(\"\\n\").split(\"\\t\")\n            if len(cols) < 2: continue\n            src, tgt = cols[0], cols[1] # English, Indonesian\n            src_tokens = normalize_and_tokenize(src)\n            tgt_tokens = normalize_and_tokenize(tgt)\n            if src_tokens and tgt_tokens:\n                pairs.append((src_tokens, tgt_tokens))\n    random.shuffle(pairs)\n    return pairs[:max_pairs]\n\ndef build_vocab(token_lists, min_freq=2):\n    \"\"\"Builds a vocabulary from a list of tokenized sentences.\"\"\"\n    counter = Counter(tok for tokens in token_lists for tok in tokens)\n    vocab = {sp: i for i, sp in enumerate(SPECIALS)}\n    for word, freq in counter.items():\n        if freq >= min_freq:\n            vocab[word] = len(vocab)\n    itos = {i: w for w, i in vocab.items()}\n    return vocab, itos\n\ndef to_ids(tokens, vocab):\n    \"\"\"Converts a list of tokens to a list of IDs.\"\"\"\n    return [BOS_IDX] + [vocab.get(t, UNK_IDX) for t in tokens] + [EOS_IDX]\n\nclass NMTDataset(Dataset):\n    \"\"\"Custom PyTorch Dataset for NMT.\"\"\"\n    def __init__(self, pairs, src_vocab, trg_vocab):\n        self.data = []\n        for src, trg in pairs:\n            src_ids = torch.tensor(to_ids(src, src_vocab), dtype=torch.long)\n            trg_ids = torch.tensor(to_ids(trg, trg_vocab), dtype=torch.long)\n            self.data.append((src_ids, trg_ids))\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\ndef collate_batch(batch):\n    \"\"\"Pads sequences in a batch to the same length.\"\"\"\n    src_list, trg_list = [], []\n    for _src, _trg in batch:\n        src_list.append(_src)\n        trg_list.append(_trg)\n    src_pad = nn.utils.rnn.pad_sequence(src_list, padding_value=PAD_IDX)\n    trg_pad = nn.utils.rnn.pad_sequence(trg_list, padding_value=PAD_IDX)\n    return src_pad, trg_pad\n\n# --- Execute Data Preparation ---\npairs = load_pairs(\"ind.txt\", max_pairs=15000)\n\n# Split data: 80% train, 10% validation, 10% test\nn_train = int(len(pairs) * 0.8)\nn_val = int(len(pairs) * 0.1)\ntrain_pairs, val_pairs, test_pairs = pairs[:n_train], pairs[n_train:n_train+n_val], pairs[n_train+n_val:]\n\n# Build vocabularies from training data\nen_vocab, en_itos = build_vocab([p[0] for p in train_pairs])\nid_vocab, id_itos = build_vocab([p[1] for p in train_pairs])\n\n# Create DataLoaders\nBATCH_SIZE = 64\ntrain_loader = DataLoader(NMTDataset(train_pairs, en_vocab, id_vocab), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\nval_loader = DataLoader(NMTDataset(val_pairs, en_vocab, id_vocab), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\ntest_loader = DataLoader(NMTDataset(test_pairs, en_vocab, id_vocab), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n\nprint(f\"✅ Data prepared: {len(train_pairs)} train, {len(val_pairs)} val, {len(test_pairs)} test pairs.\")\nprint(f\"   English vocab: {len(en_vocab)} | Indonesian vocab: {len(id_vocab)}\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:30:10.247115Z","iopub.execute_input":"2025-08-21T16:30:10.247595Z","iopub.status.idle":"2025-08-21T16:30:10.873210Z","shell.execute_reply.started":"2025-08-21T16:30:10.247573Z","shell.execute_reply":"2025-08-21T16:30:10.872519Z"}},"outputs":[{"name":"stdout","text":"✅ Data prepared: 11904 train, 1488 val, 1489 test pairs.\n   English vocab: 2614 | Indonesian vocab: 2923\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# =============================================================================\n# STEP 3: MODEL DEFINITIONS\n# =============================================================================\n# This section contains the PyTorch classes for both the RNN and Transformer models.\n\n# -----------------------------------------------------\n# 3.1 Baseline: RNN with Bahdanau Attention\n# -----------------------------------------------------\nclass BahdanauEncoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.gru = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True)\n        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, src):\n        embedded = self.dropout(self.embedding(src))\n        outputs, hidden = self.gru(embedded)\n        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n        return outputs, hidden\n\nclass BahdanauAttention(nn.Module):\n    def __init__(self, enc_hid_dim, dec_hid_dim):\n        super().__init__()\n        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n\n    def forward(self, hidden, encoder_outputs):\n        batch_size = encoder_outputs.shape[1]\n        src_len = encoder_outputs.shape[0]\n        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n        attention = self.v(energy).squeeze(2)\n        return torch.softmax(attention, dim=1)\n\nclass BahdanauDecoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n        super().__init__()\n        self.output_dim = output_dim\n        self.attention = attention\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.gru = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input, hidden, encoder_outputs):\n        input = input.unsqueeze(0)\n        embedded = self.dropout(self.embedding(input))\n        a = self.attention(hidden, encoder_outputs).unsqueeze(1)\n        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n        weighted = torch.bmm(a, encoder_outputs).permute(1, 0, 2)\n        rnn_input = torch.cat((embedded, weighted), dim=2)\n        output, hidden = self.gru(rnn_input, hidden.unsqueeze(0))\n        prediction = self.fc_out(torch.cat((output.squeeze(0), weighted.squeeze(0), embedded.squeeze(0)), dim=1))\n        return prediction, hidden.squeeze(0)\n\nclass Seq2SeqRNN(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        trg_len, batch_size = trg.shape\n        trg_vocab_size = self.decoder.output_dim\n        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n        encoder_outputs, hidden = self.encoder(src)\n        input = trg[0,:]\n        for t in range(1, trg_len):\n            output, hidden = self.decoder(input, hidden, encoder_outputs)\n            outputs[t] = output\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input = trg[t] if teacher_force else top1\n        return outputs\n\n    def greedy_decode(self, src, max_len=50):\n        with torch.no_grad():\n            encoder_outputs, hidden = self.encoder(src)\n            ys = torch.ones(1, src.shape[1]).fill_(BOS_IDX).long().to(self.device)\n            for _ in range(max_len - 1):\n                input_t = ys[-1, :]\n                output, hidden = self.decoder(input_t, hidden, encoder_outputs)\n                pred_token = output.argmax(1)\n                ys = torch.cat([ys, pred_token.unsqueeze(0)], dim=0)\n                if (pred_token == EOS_IDX).all(): break\n        return ys\n\n# -----------------------------------------------------\n# 3.2 Advanced: Transformer\n# -----------------------------------------------------\nclass PositionalEncoding(nn.Module):\n    def __init__(self, emb_size, dropout, maxlen=5000):\n        super().__init__()\n        den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n        pos_embedding = torch.zeros((maxlen, emb_size))\n        pos_embedding[:, 0::2] = torch.sin(pos * den)\n        pos_embedding[:, 1::2] = torch.cos(pos * den)\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('pos_embedding', pos_embedding.unsqueeze(-2))\n\n    def forward(self, token_embedding):\n        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n\nclass TokenEmbedding(nn.Module):\n    def __init__(self, vocab_size, emb_size):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_size)\n        self.emb_size = emb_size\n    def forward(self, tokens):\n        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n\nclass Seq2SeqTransformer(nn.Module):\n    def __init__(self, num_enc_layers, num_dec_layers, emb_size, nhead,\n                 src_vocab_size, tgt_vocab_size, dim_feedforward=512, dropout=0.1):\n        super().__init__()\n        self.transformer = nn.Transformer(d_model=emb_size, nhead=nhead,\n                                          num_encoder_layers=num_enc_layers,\n                                          num_decoder_layers=num_dec_layers,\n                                          dim_feedforward=dim_feedforward, dropout=dropout)\n        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n\n    def forward(self, src, trg, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\n        src_emb = self.positional_encoding(self.src_tok_emb(src))\n        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n        return self.generator(outs)\n\n    def greedy_decode(self, src, max_len=50):\n        with torch.no_grad():\n            src_mask = torch.zeros((src.shape[0], src.shape[0]), device=src.device).type(torch.bool)\n            memory = self.transformer.encoder(self.positional_encoding(self.src_tok_emb(src)), src_mask)\n            ys = torch.ones(1, src.shape[1]).fill_(BOS_IDX).long().to(src.device)\n            for _ in range(max_len - 1):\n                tgt_mask = (nn.Transformer.generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(src.device)\n                out = self.transformer.decoder(self.positional_encoding(self.tgt_tok_emb(ys)), memory, tgt_mask)\n                prob = self.generator(out[-1, :, :])\n                _, next_word = torch.max(prob, dim=1)\n                ys = torch.cat([ys, next_word.unsqueeze(0)], dim=0)\n                if (next_word == EOS_IDX).all(): break\n        return ys\n\nprint(\"✅ Model classes defined.\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:30:10.874182Z","iopub.execute_input":"2025-08-21T16:30:10.874452Z","iopub.status.idle":"2025-08-21T16:30:10.897465Z","shell.execute_reply.started":"2025-08-21T16:30:10.874427Z","shell.execute_reply":"2025-08-21T16:30:10.896663Z"}},"outputs":[{"name":"stdout","text":"✅ Model classes defined.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# =============================================================================\n# STEP 4: TRAINING & EVALUATION UTILITIES\n# =============================================================================\n# This section contains helper functions for training, evaluation, and decoding.\n\ndef create_mask(src, tgt, device):\n    \"\"\"Creates masks for the Transformer model.\"\"\"\n    src_seq_len, tgt_seq_len = src.shape[0], tgt.shape[0]\n    tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_seq_len, device)\n    src_mask = torch.zeros((src_seq_len, src_seq_len), device=device).type(torch.bool)\n    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n\ndef train_epoch(model, loader, optimizer, criterion, clip, is_transformer=False):\n    model.train()\n    epoch_loss = 0\n    for src, trg in tqdm(loader, desc=\"Training\"):\n        src, trg = src.to(device), trg.to(device)\n        optimizer.zero_grad()\n        if is_transformer:\n            trg_input = trg[:-1, :]\n            src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = create_mask(src, trg_input, device)\n            logits = model(src, trg_input, src_mask, tgt_mask, src_pad_mask, tgt_pad_mask, src_pad_mask)\n            trg_out = trg[1:, :].reshape(-1)\n            logits = logits.reshape(-1, logits.shape[-1])\n        else: # RNN\n            logits = model(src, trg)\n            # FIX: Slice logits to match target shape, avoiding the ValueError\n            trg_out = trg[1:, :].reshape(-1)\n            logits = logits[1:].reshape(-1, logits.shape[-1])\n        loss = criterion(logits, trg_out)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        epoch_loss += loss.item()\n    return epoch_loss / len(loader)\n\ndef evaluate_epoch(model, loader, criterion, is_transformer=False):\n    model.eval()\n    epoch_loss = 0\n    with torch.no_grad():\n        for src, trg in tqdm(loader, desc=\"Evaluating\"):\n            src, trg = src.to(device), trg.to(device)\n            if is_transformer:\n                trg_input = trg[:-1, :]\n                src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = create_mask(src, trg_input, device)\n                logits = model(src, trg_input, src_mask, tgt_mask, src_pad_mask, tgt_pad_mask, src_pad_mask)\n                trg_out = trg[1:, :].reshape(-1)\n                logits = logits.reshape(-1, logits.shape[-1])\n            else: # RNN\n                logits = model(src, trg, teacher_forcing_ratio=0.0)\n                # FIX: Slice logits to match target shape\n                trg_out = trg[1:, :].reshape(-1)\n                logits = logits[1:].reshape(-1, logits.shape[-1])\n            loss = criterion(logits, trg_out)\n            epoch_loss += loss.item()\n    return epoch_loss / len(loader)\n\ndef decode_ids(ids, itos):\n    \"\"\"Converts a tensor of IDs back to a string.\"\"\"\n    tokens = []\n    for tok_id in ids:\n        tok = tok_id.item()\n        if tok == EOS_IDX: break\n        if tok not in {BOS_IDX, PAD_IDX}:\n            tokens.append(itos.get(tok, \"<unk>\"))\n    return \" \".join(tokens)\n\ndef calculate_bleu(model, loader, id_itos, device):\n    \"\"\"Calculates SacreBLEU score for the model on a given dataset.\"\"\"\n    model.eval()\n    hypotheses, references = [], []\n    with torch.no_grad():\n        for src, trg in loader:\n            src, trg = src.to(device), trg.to(device)\n            pred_ids = model.greedy_decode(src)\n            for b in range(src.size(1)):\n                hypotheses.append(decode_ids(pred_ids[:, b], id_itos))\n                references.append([decode_ids(trg[:, b], id_itos)])\n    return sacrebleu.corpus_bleu(hypotheses, references).score\n\nprint(\"✅ Utility functions defined.\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:30:10.898632Z","iopub.execute_input":"2025-08-21T16:30:10.898834Z","iopub.status.idle":"2025-08-21T16:30:10.918385Z","shell.execute_reply.started":"2025-08-21T16:30:10.898818Z","shell.execute_reply":"2025-08-21T16:30:10.917701Z"}},"outputs":[{"name":"stdout","text":"✅ Utility functions defined.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# =============================================================================\n# STEP 5: MAIN EXECUTION\n# =============================================================================\n# This is the main block to instantiate and train the models.\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"🚀 Using device: {device}\")\n\n# --- Hyperparameters ---\nN_EPOCHS = 30\nCLIP = 1.0\nLEARNING_RATE = 0.0005\n\n# --- Train and Evaluate RNN Baseline ---\nprint(\"\\n--- Training Baseline RNN + Attention ---\")\nENC_EMB_DIM = 256\nDEC_EMB_DIM = 256\nENC_HID_DIM = 512\nDEC_HID_DIM = 512\nDROPOUT = 0.5\n\nattn_rnn = BahdanauAttention(ENC_HID_DIM, DEC_HID_DIM)\n# FIX: Corrected DEC_HID_dim to DEC_HID_DIM\nencoder_rnn = BahdanauEncoder(len(en_vocab), ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DROPOUT)\n# FIX: Corrected DEC_HID_dim to DEC_HID_DIM\ndecoder_rnn = BahdanauDecoder(len(id_vocab), DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DROPOUT, attn_rnn)\nmodel_rnn = Seq2SeqRNN(encoder_rnn, decoder_rnn, device).to(device)\n\noptimizer_rnn = optim.Adam(model_rnn.parameters(), lr=LEARNING_RATE)\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n\nfor epoch in range(N_EPOCHS):\n    train_loss = train_epoch(model_rnn, train_loader, optimizer_rnn, criterion, CLIP)\n    val_loss = evaluate_epoch(model_rnn, val_loader, criterion)\n    print(f\"Epoch {epoch+1:02} | Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}\")\n\n# --- Train and Evaluate Transformer ---\nprint(\"\\n--- Training Transformer ---\")\nEMB_SIZE = 512\nNHEAD = 8\nFFN_HID_DIM = 512\nNUM_ENC_LAYERS = 3\nNUM_DEC_LAYERS = 3\n\nmodel_transformer = Seq2SeqTransformer(NUM_ENC_LAYERS, NUM_DEC_LAYERS, EMB_SIZE, NHEAD,\n                                       len(en_vocab), len(id_vocab), FFN_HID_DIM).to(device)\noptimizer_transformer = optim.Adam(model_transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n\nfor epoch in range(N_EPOCHS):\n    train_loss = train_epoch(model_transformer, train_loader, optimizer_transformer, criterion, CLIP, is_transformer=True)\n    val_loss = evaluate_epoch(model_transformer, val_loader, criterion, is_transformer=True)\n    print(f\"Epoch {epoch+1:02} | Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}\")\n\n# --- Final Evaluation ---\nprint(\"\\n--- Final Evaluation on Test Set ---\")\nbleu_rnn = calculate_bleu(model_rnn, test_loader, id_itos, device)\nbleu_transformer = calculate_bleu(model_transformer, test_loader, id_itos, device)\nprint(f\"🏆 Final BLEU Score (RNN Baseline): {bleu_rnn:.2f}\")\nprint(f\"🏆 Final BLEU Score (Transformer): {bleu_transformer:.2f}\")\n\n# --- Show Example Translations ---\ndef show_examples(model, loader, en_itos, id_itos, n=3):\n    print(\"\\n--- Example Translations ---\")\n    model.eval()\n    with torch.no_grad():\n        for i, (src, trg) in enumerate(loader):\n            if i >= n: break\n            src, trg = src.to(device), trg.to(device)\n            pred_ids = model.greedy_decode(src)\n            src_text = decode_ids(src[:, 0], en_itos)\n            trg_text = decode_ids(trg[:, 0], id_itos)\n            pred_text = decode_ids(pred_ids[:, 0], id_itos)\n            print(f\"\\n  SRC:  {src_text}\")\n            print(f\"  TRG:  {trg_text}\")\n            print(f\"  PRED: {pred_text}\")\n\nshow_examples(model_transformer, test_loader, en_itos, id_itos)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:47:05.482568Z","iopub.execute_input":"2025-08-21T16:47:05.482908Z","iopub.status.idle":"2025-08-21T16:54:58.758603Z","shell.execute_reply.started":"2025-08-21T16:47:05.482885Z","shell.execute_reply":"2025-08-21T16:54:58.757991Z"}},"outputs":[{"name":"stdout","text":"🚀 Using device: cuda\n\n--- Training Baseline RNN + Attention ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:09<00:00, 20.58it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 62.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | Train Loss: 4.526 | Val Loss: 3.752\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:09<00:00, 20.45it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 62.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 02 | Train Loss: 3.287 | Val Loss: 2.958\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:09<00:00, 20.52it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 62.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 03 | Train Loss: 2.571 | Val Loss: 2.648\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:09<00:00, 20.65it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 62.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 04 | Train Loss: 2.112 | Val Loss: 2.499\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:09<00:00, 20.56it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 62.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 05 | Train Loss: 1.802 | Val Loss: 2.486\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:09<00:00, 20.57it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 62.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 06 | Train Loss: 1.548 | Val Loss: 2.430\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:09<00:00, 20.56it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 62.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 07 | Train Loss: 1.375 | Val Loss: 2.408\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:09<00:00, 20.40it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 62.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 08 | Train Loss: 1.220 | Val Loss: 2.426\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:09<00:00, 20.55it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 59.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 09 | Train Loss: 1.107 | Val Loss: 2.402\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:09<00:00, 20.32it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 62.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Train Loss: 1.023 | Val Loss: 2.407\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:09<00:00, 20.29it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 63.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 | Train Loss: 0.926 | Val Loss: 2.508\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:09<00:00, 20.43it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 63.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 | Train Loss: 0.837 | Val Loss: 2.513\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:09<00:00, 20.51it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 61.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 | Train Loss: 0.785 | Val Loss: 2.501\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:08<00:00, 20.71it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 62.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 | Train Loss: 0.726 | Val Loss: 2.607\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:09<00:00, 20.58it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 63.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15 | Train Loss: 0.663 | Val Loss: 2.647\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:09<00:00, 20.56it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 62.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16 | Train Loss: 0.615 | Val Loss: 2.693\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:09<00:00, 20.66it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 63.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17 | Train Loss: 0.561 | Val Loss: 2.705\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:09<00:00, 20.44it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 62.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18 | Train Loss: 0.536 | Val Loss: 2.724\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:09<00:00, 20.21it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 61.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19 | Train Loss: 0.502 | Val Loss: 2.795\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:09<00:00, 20.30it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 62.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20 | Train Loss: 0.478 | Val Loss: 2.819\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:09<00:00, 20.37it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 63.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21 | Train Loss: 0.441 | Val Loss: 2.846\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:09<00:00, 20.41it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 62.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22 | Train Loss: 0.418 | Val Loss: 2.835\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:09<00:00, 20.60it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 62.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23 | Train Loss: 0.392 | Val Loss: 2.911\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:09<00:00, 19.99it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 62.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24 | Train Loss: 0.369 | Val Loss: 2.981\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:09<00:00, 20.40it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 62.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25 | Train Loss: 0.354 | Val Loss: 3.013\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:09<00:00, 20.35it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 61.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26 | Train Loss: 0.343 | Val Loss: 3.037\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:09<00:00, 20.43it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 62.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27 | Train Loss: 0.331 | Val Loss: 3.160\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:09<00:00, 20.44it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 62.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28 | Train Loss: 0.326 | Val Loss: 3.081\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:09<00:00, 20.40it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 61.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29 | Train Loss: 0.300 | Val Loss: 3.162\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:08<00:00, 20.71it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 62.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30 | Train Loss: 0.287 | Val Loss: 3.162\n\n--- Training Transformer ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 32.11it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 123.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | Train Loss: 4.669 | Val Loss: 3.875\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 32.16it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 123.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 02 | Train Loss: 3.874 | Val Loss: 3.502\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 32.29it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 123.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 03 | Train Loss: 3.511 | Val Loss: 3.225\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 32.26it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 124.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 04 | Train Loss: 3.228 | Val Loss: 3.017\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 32.23it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 123.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 05 | Train Loss: 2.995 | Val Loss: 2.860\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 32.13it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 123.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 06 | Train Loss: 2.799 | Val Loss: 2.736\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 32.08it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 123.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 07 | Train Loss: 2.619 | Val Loss: 2.608\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 31.97it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 123.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 08 | Train Loss: 2.468 | Val Loss: 2.504\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 31.94it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 123.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 09 | Train Loss: 2.316 | Val Loss: 2.420\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 31.86it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 124.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Train Loss: 2.179 | Val Loss: 2.338\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 31.96it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 123.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 | Train Loss: 2.054 | Val Loss: 2.297\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 32.16it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 124.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 | Train Loss: 1.936 | Val Loss: 2.205\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 32.05it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 123.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 | Train Loss: 1.826 | Val Loss: 2.159\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 32.09it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 124.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 | Train Loss: 1.725 | Val Loss: 2.127\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 32.30it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 123.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15 | Train Loss: 1.631 | Val Loss: 2.054\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 32.27it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 123.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16 | Train Loss: 1.535 | Val Loss: 2.030\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 32.24it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 124.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17 | Train Loss: 1.444 | Val Loss: 1.979\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 32.40it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 123.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18 | Train Loss: 1.363 | Val Loss: 1.975\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 32.12it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 124.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19 | Train Loss: 1.282 | Val Loss: 1.957\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 32.24it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 123.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20 | Train Loss: 1.205 | Val Loss: 1.960\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 32.10it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 124.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21 | Train Loss: 1.137 | Val Loss: 1.905\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 32.10it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 124.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22 | Train Loss: 1.068 | Val Loss: 1.909\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 32.02it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 123.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23 | Train Loss: 1.007 | Val Loss: 1.887\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 32.08it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 123.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24 | Train Loss: 0.940 | Val Loss: 1.913\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 32.23it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 123.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25 | Train Loss: 0.887 | Val Loss: 1.886\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 32.04it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 123.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26 | Train Loss: 0.836 | Val Loss: 1.883\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 32.08it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 123.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27 | Train Loss: 0.784 | Val Loss: 1.900\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 32.21it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 123.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28 | Train Loss: 0.733 | Val Loss: 1.877\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 32.19it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 124.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29 | Train Loss: 0.693 | Val Loss: 1.890\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 186/186 [00:05<00:00, 32.14it/s]\nEvaluating: 100%|██████████| 24/24 [00:00<00:00, 124.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30 | Train Loss: 0.645 | Val Loss: 1.881\n\n--- Final Evaluation on Test Set ---\n🏆 Final BLEU Score (RNN Baseline): 84.09\n🏆 Final BLEU Score (Transformer): 18.00\n\n--- Example Translations ---\n\n  SRC:  i know tom will cry .\n  TRG:  saya tahu tom akan menangis .\n  PRED: aku tahu kalau tom jauh lebih dari jauh dari jauh dari jauh .\n\n  SRC:  i must decline .\n  TRG:  aku harus menolak .\n  PRED: aku harus jauh dari jauh .\n\n  SRC:  it s a pity that you can t join us .\n  TRG:  sangat <unk> anda tidak dapat bergabung dengan kami .\n  PRED: sayang sekali kamu tidak bisa bergabung dengan kami .\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"def translate_sentence(model, sentence, en_vocab, id_itos, device, max_len=50):\n    \"\"\"Menerjemahkan satu kalimat string menggunakan model yang sudah dilatih.\"\"\"\n    model.eval()  # Set model ke mode evaluasi\n\n    # 1. Tokenisasi dan konversi ke ID\n    tokens = normalize_and_tokenize(sentence)\n    ids = to_ids(tokens, en_vocab)\n    \n    # 2. Konversi ke tensor dan tambahkan dimensi batch (batch_size=1)\n    src_tensor = torch.LongTensor(ids).unsqueeze(1).to(device)\n\n    # 3. Lakukan prediksi dengan greedy_decode\n    with torch.no_grad():\n        pred_ids = model.greedy_decode(src_tensor, max_len=max_len)\n\n    # 4. Konversi ID hasil prediksi kembali ke teks\n    translation = decode_ids(pred_ids[:, 0], id_itos)\n    \n    return translation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:56:01.813844Z","iopub.execute_input":"2025-08-21T16:56:01.814237Z","iopub.status.idle":"2025-08-21T16:56:01.819076Z","shell.execute_reply.started":"2025-08-21T16:56:01.814214Z","shell.execute_reply":"2025-08-21T16:56:01.818379Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# --- Coba terjemahkan kalimat baru ---\nprint(\"\\n--- Interactive Test ---\")\n\nkalimat_tes_1 = \"Bad boy.\"\nkalimat_tes_2 = \"My name is Tom and I live in a small city.\"\n\n# Menggunakan model Transformer\nterjemahan_1 = translate_sentence(model_transformer, kalimat_tes_1, en_vocab, id_itos, device)\nterjemahan_2 = translate_sentence(model_transformer, kalimat_tes_2, en_vocab, id_itos, device)\n\nprint(f\"\\nModel: Transformer\")\nprint(f\"English: {kalimat_tes_1}\")\nprint(f\"Indonesian: {terjemahan_1}\")\n\nprint(f\"\\nEnglish: {kalimat_tes_2}\")\nprint(f\"Indonesian: {terjemahan_2}\")\n\n\n# Menggunakan model RNN (sebagai perbandingan)\nterjemahan_rnn = translate_sentence(model_rnn, kalimat_tes_1, en_vocab, id_itos, device)\nprint(f\"\\nModel: RNN Baseline\")\nprint(f\"English: {kalimat_tes_1}\")\nprint(f\"Indonesian: {terjemahan_rnn}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T16:58:47.161104Z","iopub.execute_input":"2025-08-21T16:58:47.161379Z","iopub.status.idle":"2025-08-21T16:58:47.221087Z","shell.execute_reply.started":"2025-08-21T16:58:47.161358Z","shell.execute_reply":"2025-08-21T16:58:47.220485Z"}},"outputs":[{"name":"stdout","text":"\n--- Interactive Test ---\n\nModel: Transformer\nEnglish: Bad boy.\nIndonesian: anak laki laki yang buruk .\n\nEnglish: My name is Tom and I live in a small city.\nIndonesian: saya tinggal di depan saya tinggal .\n\nModel: RNN Baseline\nEnglish: Bad boy.\nIndonesian: buruk .\n","output_type":"stream"}],"execution_count":18}]}